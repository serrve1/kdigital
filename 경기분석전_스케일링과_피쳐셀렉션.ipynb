{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1LYILE_UQsKCfBSzLInspQMVo-8UhKpfg",
      "authorship_tag": "ABX9TyPA3ObTtmG/kPQ2nQZBGiGj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serrve1/kdigital/blob/main/%EA%B2%BD%EA%B8%B0%EB%B6%84%EC%84%9D%EC%A0%84_%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%A7%81%EA%B3%BC_%ED%94%BC%EC%B3%90%EC%85%80%EB%A0%89%EC%85%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "경기지표를 보고 경기국면을 분석한다는 말은\n",
        "경기 국면에 따라 자산군의 움직임에 영향을 주기 때문임.\n",
        "- 국면이 자주바뀐다거나 이상하게 나오면\n",
        "- 데이터를 잘못 다뤘을 가능성이 크다./ 이건 나중에 결과를 보고 판단하면 됨.\n",
        "-  SAA 전략을 세울 때 자주 리벨런싱하지 않고\n",
        "자산군의 형태를 잘 파악할 수 있도록 하기\n",
        "\n",
        "- 안되면 회귀분석을 통해 유의한 피쳐만 골라내고\n",
        "  경기적 패턴을 분석(경기성장 배제, 경기순환 (잡기힘듬) ==> 알려진 시클리컬 지표들만을 이용하여 경기변동분석)\n",
        "\n",
        "\n",
        "#### 어떠한 데이터 프레임이 들어오면 => 시각화 형태를 보고 => 적절한 스케일링\n",
        "#### 트레인 테스트 셋을 나눈 후 =>   피쳐 셀렉션하기 / 여러 방법을 통해서\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GwYbZ_7FGo7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 스케일링\n",
        "\n",
        "1. min-max : 0~1까지인데 이건 시간적으로 상승하는데이터는\n",
        "좋지 않아보임\n",
        "\n",
        "2. StandardScaler : 정규화 이게 그나마 나은데\n",
        "정규분포가 있다는 가정하에 쓰는 것이므로 정규성을 먼저 확인하는게 맞을 것 같은데 확인해볼 것\n",
        "\n",
        "3."
      ],
      "metadata": {
        "id": "INVwy9T5HvBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "# 한글폰트\n",
        "# 나눔고딕 폰트 설치\n",
        "!apt-get -qq -y install fonts-nanum > /dev/null\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "\n",
        "# 한글 폰트 설정\n",
        "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
        "plt.rc('font', family=font_name)\n",
        "\n",
        "# 런타임 다시 시작 (이 작업은 Colab에서 한 번만 수행하면 됨)\n",
        "# Runtime -> Restart runtime 선택하거나 Ctrl+M . 입력 후 선택\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gorhmm5gWsH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# 경고 메시지를 출력하지 않도록 설정\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 경고 메시지를 발생시키는 함수\n",
        "def example_function():\n",
        "    warnings.warn(\"이 함수는 예제일 뿐입니다.\", UserWarning)\n",
        "\n",
        "# 함수 호출\n",
        "example_function()"
      ],
      "metadata": {
        "id": "bl3fxxKHNg2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUELVSiZFmUV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 스케일링\n",
        "데이터 스케일링(Data Scaling)이란 서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업을 의미합니다. 값을 조정하는 과정이기 때문에 수치형 변수에만 적용해야 합니다.\n",
        "\n",
        "사이킷런에서는 스케일링을 수행하기 위한 다양한 스케일러를 제공하는데요.\n",
        "이때 모든 스케일러는 공통적으로 다음과 같은 메서드를 이용합니다.\n",
        "\n",
        "fit(): 데이터 변환을 위한 기존 정보 설정 (ex: 데이터 세트의 최댓값/최솟값)\n",
        "transform(): fit()을 통해 설정된 정보를 이용해 실제로 데이터를 변환\n",
        "그리고 fit_transform()은 위 두 가지 메서드를 한 번에 적용하는 기능을 수행합니다.\n",
        "\n",
        "\n",
        "우선, 스케일링의 대표적인 방법인 표준화(Standardization)와 정규화(Normalization)를 살펴보겠습니다."
      ],
      "metadata": {
        "id": "24r8jHQH3dtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본패키지 설치\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans # 케이민즈 알고리즘\n",
        "from sklearn.preprocessing import StandardScaler# 스텐다드 스케일러\n",
        "# 한글 폰트 사용을 위해서 세팅\n",
        "from matplotlib import font_manager, rc"
      ],
      "metadata": {
        "id": "t_pumUnIHZkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터프레임 불러오기\n",
        "economics = pd.read_csv('/content/drive/MyDrive/프로젝트1/dataset/resampled_data_final.csv',encoding=\"euc-kr\")\n",
        "economics['DATE'] = pd.to_datetime(economics['DATE'])\n",
        "economics['DATE'].info()"
      ],
      "metadata": {
        "id": "WdT221hGHwaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "economics[economics.isna().any(axis=1)] # 결측치 확인"
      ],
      "metadata": {
        "id": "wPNsZ_xbN0Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "economics = economics.iloc[:287 , ] # 결측치 제거\n",
        "economics[economics.isna().any(axis=1)] # 결측치 확인"
      ],
      "metadata": {
        "id": "UtZtItjkOgTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kb-kXsYNRi3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "economics = economics.iloc[:240] # 2019년까지만"
      ],
      "metadata": {
        "id": "A2EkieQnScQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 주택가격지수. 제조업신규주문 , 경기동행종합지수는 차분하고 (변동치로 변경한다는 뜻)\n",
        "# 제조업 신규주문은 앞으로 어떻게 처리할지 일단 보류\n",
        "#M2와 소매판매는 변화율로 바꾸는 전처리 시행\n",
        "economics.주택가격지수 = economics.주택가격지수.diff().fillna(method= 'bfill')\n",
        "economics.경기동행종합 = economics.경기동행종합.diff().fillna(method= 'bfill')\n",
        "economics['M2차분'] = economics.M2통화량.diff().fillna(method= 'bfill')\n",
        "economics['M2변화율'] = economics.M2차분/ (economics.M2통화량.shift(1)) * 100\n",
        "#economics['M2변화율'] .fillna(method = 'ffill')\n",
        "economics['소매판매차분'] = economics.소매판매.diff().fillna(method='bfill')\n",
        "economics['소매판매변화율'] = economics.소매판매차분/ (economics.소매판매.shift(1)) * 100\n",
        "\n",
        "\n",
        "# 차분, 변화율 확인\n",
        "economics[['M2통화량','M2차분','M2변화율','소매판매','소매판매차분','소매판매변화율']].head(10)\n",
        "# 결측치를 다음날 데이터로 채워주기\n",
        "economics['M2변화율'] = economics['M2변화율'].fillna(method = 'bfill')\n",
        "economics['소매판매변화율'] = economics['소매판매변화율'].fillna(method = 'bfill')\n",
        "# M2, 소매판매 원데이터를 지웠습니다.\n",
        "economics.drop(['M2통화량','M2차분','소매판매','소매판매차분'],axis = 1,inplace = True)"
      ],
      "metadata": {
        "id": "Y1Ntxejgg0yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KvvAs74krYLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "economics = economics.set_index('DATE')"
      ],
      "metadata": {
        "id": "aA1fbiLGVHvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프 크기 조정\n",
        "plt.rc('font', family='NanumGothic')\n",
        "가로, 세로 = 30, 20  # 원하는 크기로 조절\n",
        "fig, axs = plt.subplots(nrows=5, ncols=4, figsize=(가로, 세로))  # 3행 4열의 서브 플롯 생성\n",
        "# 각 컬럼에 대한 서브 플롯 그리기\n",
        "for i, col in enumerate(economics.columns):\n",
        "    economics.plot(y= col, kind='line', ax=axs[i//4, i%4], color='blue')\n",
        "    axs[i//4, i%4].set_title(f'{col} 서브 플롯')\n",
        "\n",
        "# 그래프 출력\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lXLtg-kqPhzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "표준화(Standardization)는 변수 각각의 평균을 0, 분산을 1로 만들어주는 스케일링 기법입니다. 표준화가 적용된 변수는 가우시안 정규분포를 가진 값으로 변환됩니다.\n",
        "\n",
        "아래 수식과 같이, 변수 x의 원래 값에서 x의 평균을 뺀 값을 x의 표준편차로 나눈 값으로 계산할 수 있습니다.\n"
      ],
      "metadata": {
        "id": "3J_KKaKpXhec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scikit-learn 패키지의 StandardScaler 클래스를 불러옵니다.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# StandardScaler 객체를 생성합니다.\n",
        "standard_scaler = StandardScaler()\n",
        "# fit_transform()을 사용해서 학습과 스케일링을 한 번에 적용합니다.\n",
        "scaled_eocs = standard_scaler.fit_transform(economics)\n",
        "# 표준화가 완료된 데이터를 데이터프레임 형태로 변환합니다.\n",
        "scaled_eocs = pd.DataFrame(scaled_eocs,\n",
        "                              index=economics.index,\n",
        "                              columns=economics.columns)\n",
        "# 표준화가 잘 되었는지 데이터를 확인해봅시다.\n",
        "scaled_eocs.head()"
      ],
      "metadata": {
        "id": "MLHxg_BxXdrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 일단 MIN-MAX로 진행하기로 하였기 때문에 주석처리하였음\n",
        "\n",
        "# # scikit-learn 패키지의 StandardScaler 클래스를 불러옵니다.\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# # StandardScaler 객체를 생성합니다.\n",
        "# standard_scaler = StandardScaler()\n",
        "# # fit_transform()을 사용해서 학습과 스케일링을 한 번에 적용합니다.\n",
        "# scaled_eocs = standard_scaler.fit_transform(economics)\n",
        "# # 표준화가 완료된 데이터를 데이터프레임 형태로 변환합니다.\n",
        "# scaled_eocs = pd.DataFrame(scaled_eocs,\n",
        "#                               index=economics.index,\n",
        "#                               columns=economics.columns)\n",
        "# # 표준화가 잘 되었는지 데이터를 확인해봅시다.\n",
        "# scaled_eocs.head()"
      ],
      "metadata": {
        "id": "x497GANadgaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 민멕스 스케일링\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# MinMaxScaler 선언 및 Fitting\n",
        "mMscaler = MinMaxScaler()\n",
        "mMscaler.fit(economics)\n",
        "\n",
        "# 데이터 변환\n",
        "mMscaled_data = mMscaler.transform(economics)\n",
        "\n",
        "# 데이터 프레임으로 저장\n",
        "mMscaled_data = pd.DataFrame(economics)"
      ],
      "metadata": {
        "id": "TMzxf6lpdrYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.cluster import KMeans\n",
        "# kmeans = KMeans(n_clusters =4, init = 'k-means++', max_iter= 300, random_state= 0) # 군집을 4개로\n",
        "# kmeans.fit(mMscaled_data)"
      ],
      "metadata": {
        "id": "4jbaqbzVbIcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "import plotly.express as px\n"
      ],
      "metadata": {
        "id": "WFTarhapcoG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "sum_of_squared_distances = []\n",
        "K = range(1, 15)\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters = k)\n",
        "    km = km.fit(mMscaled_data) # 데이터프레임 입력\n",
        "    sum_of_squared_distances.append(km.inertia_)\n",
        "\n",
        "# 시각화\n",
        "\n",
        "plt.plot(K, sum_of_squared_distances, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.axvline(2,c= 'r',alpha =0.4)\n",
        "plt.axvline(3,c= 'r',alpha =0.4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-e6irRR9c-Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "k_range = range(2,14)\n",
        "\n",
        "best_n = -1\n",
        "best_silhouette_score = -1\n",
        "\n",
        "for k in k_range:\n",
        "\n",
        "  kmeans = KMeans(n_clusters=k, random_state=200)\n",
        "  kmeans.fit(mMscaled_data)#데이터프레임\n",
        "  clusters = kmeans.predict(mMscaled_data)#데이터프레임입력\n",
        "\n",
        "  score = silhouette_score(mMscaled_data, clusters)\n",
        "  print('k :',  k, 'score :', score)\n",
        "\n",
        "  if score > best_silhouette_score:\n",
        "    best_n = k\n",
        "    best_silhouette_score = score\n",
        "\n",
        "print('best n :', best_n, 'best score :', best_silhouette_score )"
      ],
      "metadata": {
        "id": "v9prjvEWdAml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# KMeans\n",
        "kmeans = KMeans(n_clusters=6, init=\"k-means++\", max_iter=300, random_state=200)\n",
        "kmeans.fit(mMscaled_data)\n",
        "\n",
        "# 데이터 프레임에 cluster 추가\n",
        "mMscaled_data[\"cluster\"] = kmeans.labels_\n",
        "\n",
        "# 개별 데이터 실루엣 계수\n",
        "score_samples = silhouette_samples(mMscaled_data, mMscaled_data.cluster)\n",
        "print(f'silhouette_samples( ) return 값의 shape: {score_samples.shape}')\n",
        "\n",
        "# 데이터 프레임에 실루엣 계수 추가\n",
        "mMscaled_data['silhouette_coeff'] = score_samples\n",
        "\n",
        "# 모든 데이터의 평균 실루엣 계수\n",
        "average_score = silhouette_score(mMscaled_data, mMscaled_data.cluster)\n",
        "print(f'경제데이터 Silhouette Analysis Score: {average_score:.3f}')\n"
      ],
      "metadata": {
        "id": "cUtQIoBBeV85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mMscaled_data.groupby(\"cluster\").mean()[[\"silhouette_coeff\"]]"
      ],
      "metadata": {
        "id": "cAveo5PH-s0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_silhouette_sklearn(range_n_clusters, X):\n",
        "    from sklearn.datasets import make_blobs\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.cm as cm\n",
        "    import numpy as np\n",
        "\n",
        "    for n_clusters in range_n_clusters:\n",
        "        # Create a subplot with 1 row and 2 columns\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "        fig.set_size_inches(18, 7)\n",
        "\n",
        "        # The 1st subplot is the silhouette plot\n",
        "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "        # lie within [-0.1, 1]\n",
        "        ax1.set_xlim([-0.1, 1])\n",
        "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "        # plots of individual clusters, to demarcate them clearly.\n",
        "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "        # Initialize the clusterer with n_clusters value and a random generator\n",
        "        # seed of 10 for reproducibility.\n",
        "        clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "        cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "        # The silhouette_score gives the average value for all the samples.\n",
        "        # This gives a perspective into the density and separation of the formed\n",
        "        # clusters\n",
        "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "\n",
        "        # Compute the silhouette scores for each sample\n",
        "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "        y_lower = 10\n",
        "        for i in range(n_clusters):\n",
        "            # Aggregate the silhouette scores for samples belonging to\n",
        "            # cluster i, and sort them\n",
        "            ith_cluster_silhouette_values = \\\n",
        "                sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "            ith_cluster_silhouette_values.sort()\n",
        "\n",
        "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "            y_upper = y_lower + size_cluster_i\n",
        "\n",
        "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                              0, ith_cluster_silhouette_values,\n",
        "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "            # Label the silhouette plots with their cluster numbers at the middle\n",
        "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "            # Compute the new y_lower for next plot\n",
        "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "        ax1.set_title('Number of Cluster : '+ str(n_clusters)+'\\n' \\\n",
        "                              'Silhouette Score :' + str(round(silhouette_avg,3)))\n",
        "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "        ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "        # The vertical line for average silhouette score of all the values\n",
        "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "        # 2nd Plot showing the actual clusters formed\n",
        "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "        ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                    c=colors, edgecolor='k')\n",
        "\n",
        "        # Labeling the clusters\n",
        "        centers = clusterer.cluster_centers_\n",
        "        # Draw white circles at cluster centers\n",
        "        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                    c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "        for i, c in enumerate(centers):\n",
        "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                        s=50, edgecolor='k')\n",
        "\n",
        "        ax2.set_title(\"The visualization of the clustered data.\")\n",
        "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                      \"with n_clusters = %d\" % n_clusters),\n",
        "                     fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "UQCA1zfbA_SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가상데이터: 피처 2개, 클러스터 6개\n",
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(n_samples=500, n_features=2, centers=6, cluster_std=1,\n",
        "                  center_box=(-10.0, 10.0), shuffle=True, random_state=1)\n",
        "\n",
        "# K-Means K: 6\n",
        "visualize_silhouette_sklearn([6], X)\n"
      ],
      "metadata": {
        "id": "2q8aXq2rBfGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 여러개의 클러스터링 갯수를 List로 입력 받아 각각의 실루엣 계수를 면적으로 시각화한 함수 작성\n",
        "def visualize_silhouette(cluster_lists, X_features):\n",
        "\n",
        "    from sklearn.datasets import make_blobs\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.cm as cm\n",
        "    import math\n",
        "\n",
        "    # 입력값으로 클러스터링 갯수들을 리스트로 받아서, 각 갯수별로 클러스터링을 적용하고 실루엣 개수를 구함\n",
        "    n_cols = len(cluster_lists)\n",
        "\n",
        "    # plt.subplots()으로 리스트에 기재된 클러스터링 수만큼의 sub figures를 가지는 axs 생성\n",
        "    fig, axs = plt.subplots(figsize=(4*n_cols, 4), nrows=1, ncols=n_cols)\n",
        "\n",
        "    # 리스트에 기재된 클러스터링 갯수들을 차례로 iteration 수행하면서 실루엣 개수 시각화\n",
        "    for ind, n_cluster in enumerate(cluster_lists):\n",
        "\n",
        "        # KMeans 클러스터링 수행하고, 실루엣 스코어와 개별 데이터의 실루엣 값 계산.\n",
        "        clusterer = KMeans(n_clusters = n_cluster, max_iter=500, random_state=0)\n",
        "        cluster_labels = clusterer.fit_predict(X_features)\n",
        "\n",
        "        sil_avg = silhouette_score(X_features, cluster_labels)\n",
        "        sil_values = silhouette_samples(X_features, cluster_labels)\n",
        "\n",
        "        y_lower = 10\n",
        "        axs[ind].set_title('Number of Cluster : '+ str(n_cluster)+'\\n' \\\n",
        "                          'Silhouette Score :' + str(round(sil_avg,3)) )\n",
        "        axs[ind].set_xlabel(\"The silhouette coefficient values\")\n",
        "        axs[ind].set_ylabel(\"Cluster label\")\n",
        "        axs[ind].set_xlim([-0.1, 1])\n",
        "        axs[ind].set_ylim([0, len(X_features) + (n_cluster + 1) * 10])\n",
        "        axs[ind].set_yticks([])  # Clear the yaxis labels / ticks\n",
        "        axs[ind].set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "        # 클러스터링 갯수별로 fill_betweenx( )형태의 막대 그래프 표현.\n",
        "        for i in range(n_cluster):\n",
        "            ith_cluster_sil_values = sil_values[cluster_labels==i]\n",
        "            ith_cluster_sil_values.sort()\n",
        "\n",
        "            size_cluster_i = ith_cluster_sil_values.shape[0]\n",
        "            y_upper = y_lower + size_cluster_i\n",
        "\n",
        "            color = cm.nipy_spectral(float(i) / n_cluster)\n",
        "            axs[ind].fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sil_values, \\\n",
        "                                facecolor=color, edgecolor=color, alpha=0.7)\n",
        "            axs[ind].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "            y_lower = y_upper + 10\n",
        "\n",
        "        axs[ind].axvline(x=sil_avg, color=\"red\", linestyle=\"--\")"
      ],
      "metadata": {
        "id": "2GBc8p0jBwBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iris 군집 개수 최적화\n",
        "visualize_silhouette([ 2, 3, 4, 5 ], mMscaled_data)"
      ],
      "metadata": {
        "id": "oPRQux2PBzhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mMscaled_data[mMscaled_data['cluster']==0]"
      ],
      "metadata": {
        "id": "8bdWYXGjC2Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mMscaled_data"
      ],
      "metadata": {
        "id": "sAJP0Gj1C13P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}